{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the PCU-alignment-lexicon consists of three steps: extracting all words needed in the lexicon; using the G2P tool to get phonetic transcriptions of all words; and aligning the phonetic transcriptions with the words. For the first and last step, code is provided in this notebook. For the second step, the code from a Github repository can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import helper_scripts.graph_phon_alignment as gpa\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extracting all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_directory = '/vol/tensusers5/wharmsen/spelling-data/aseda/2-targetdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracts all target words from all files found in the given directory. \n",
    "The files are expected to be .csv files and to contain a 'target' column.\n",
    "\"\"\"\n",
    "def extract_all_target_words(target_directory):\n",
    "\n",
    "    all_target_words = set()\n",
    "\n",
    "    for filename in os.listdir(target_directory):\n",
    "        location = target_directory + filename\n",
    "\n",
    "        dataframe = pd.read_csv(location)\n",
    "        all_target_words.update(dataframe['target'])\n",
    "\n",
    "    return list(all_target_words)\n",
    "\n",
    "\n",
    "def write_words_wordlist(words):\n",
    "\n",
    "    words = sorted(words, key=lambda v: (v.upper(), v[0].islower()))\n",
    "    with open('all_target_words_list', \"w\", encoding='utf-8') as f:\n",
    "        for word in words:\n",
    "            f.write(word+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = extract_all_target_words(target_directory)\n",
    "write_words_wordlist(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using a G2P tool\n",
    "\n",
    "For this step, any desired G2P tool can be used. In case of using the CLST webservice, the Lexiconator can be used. This is a Python binding of the webservice and can be found [here](https://github.com/cristiantg/lexiconator). If the step above is finished, only the uber_script of the lexiconator is needed. If wished, the pre-processing step of the lexiconator can also be used instead of the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Aligning the phonemes and graphemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2p_output_path = '/vol/tensusers5/wharmsen/spelling-data/aseda/resources/lexicons/all-target-words-list/results-final/lexicon.txt'\n",
    "lexicon_output_directory =\"/vol/tensusers5/wharmsen/spelling-data/aseda/resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grapheme_phoneme_from_g2p_output(g2p_output_path):\n",
    "    with open(g2p_output_path) as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    lines = text.split('\\n')\n",
    "    lines = [x for x in lines if x!='']\n",
    "\n",
    "    #Save lexicon in grapheme and phoneme list\n",
    "    graphemes = []\n",
    "    graphemes_aligned = []\n",
    "    phonemes = []\n",
    "    phonemes_aligned = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.split(\"\\t\")\n",
    "\n",
    "        grapheme = line[0]\n",
    "        phoneme = line[1]\n",
    "        grapheme_align, phoneme_align = gpa.align_word_and_phon_trans(grapheme, phoneme)\n",
    "\n",
    "        graphemes.append(grapheme)\n",
    "        phonemes.append(phoneme)\n",
    "        graphemes_aligned.append(grapheme_align)\n",
    "        phonemes_aligned.append(phoneme_align)\n",
    "\n",
    "    return graphemes, phonemes, graphemes_aligned, phonemes_aligned\n",
    "\n",
    "def create_alignment_lexicon(graphemes, phonemes, graph_align_list, phon_align_list):\n",
    "    matrix = []\n",
    "    for idx in range(len(phonemes)):\n",
    "        row = [graphemes[idx],phonemes[idx],graph_align_list[idx], phon_align_list[idx]]\n",
    "        matrix.append(row)\n",
    "\n",
    "    lexicon_df = pd.DataFrame(matrix, columns = [\"graphemes\", \"phonemes\", \"graphemes_align\", \"phonemes_align\"])\n",
    "    lexicon_df = lexicon_df.set_index(\"graphemes\")\n",
    "    lexicon_df = lexicon_df.dropna(subset=['phonemes'])\n",
    "\n",
    "    return lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment Lexicon created:  /vol/tensusers5/wharmsen/spelling-data/aseda/resources/graph_phon_alignment_lexicon.csv\n"
     ]
    }
   ],
   "source": [
    "graphemes, phonemes, graphemes_aligned, phonemes_aligned = extract_grapheme_phoneme_from_g2p_output(g2p_output_path)\n",
    "\n",
    "# Create lexicon with grapheme-phoneme alignment\n",
    "lexiconDF = create_alignment_lexicon(graphemes, phonemes, graphemes_aligned, phonemes_aligned)\n",
    "lexicon_output_file = lexicon_output_directory + \"/graph_phon_alignment_lexicon.csv\"\n",
    "lexiconDF.to_csv(lexicon_output_file, quoting = csv.QUOTE_NONNUMERIC, quotechar='\"')\n",
    "print(\"Alignment Lexicon created: \", lexicon_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f3051c7f43280de7e24e4c9ab32a0e709053834cfe14c7b27f2e4447f3c4fa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('sasjalamachine': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
